# Package imports
from typing import Any
import torch
import pytorch_lightning as pl
from pytorch_lightning.utilities.types import STEP_OUTPUT
import torch.nn as nn
import matplotlib.pyplot as plt
from matplotlib import colors
import mlflow
from torch.jit import script

# Class and function imports
from steps.discriminator import Discriminator, Spatial_Discriminator, Temporal_Discriminator
from steps.generator import Generator, Conditioning

from steps.losses import generator_loss, discriminator_loss, grid_cell_regularizer, SSIM, classification_metrics

from utils import log_utils, config_utils

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

class GAN_Model(pl.LightningModule):
    def __init__(self):
        super().__init__()

        # PARAMETERS
        self.lr_G = 1e-4 # generator learning rate
        self.lr_D = 1e-3 # discriminator learning rate

        self.automatic_optimization = False # important to enable manual optimization

        self.generator = Generator()
        self.discriminator = Discriminator() # or Spatial_Discriminator()

    def forward(self, context_batch):
        # forward is generating one batch of satellite images
        gen_sequence = self.generator(context_batch)
        return gen_sequence
    
    def configure_optimizers(self):
        G_Optim = torch.optim.AdamW(self.generator.parameters(), lr = self.lr_G)
        D_Optim = torch.optim.AdamW(self.discriminator.parameters(), lr = self.lr_D)
        lr_scheduler_G = torch.optim.lr_scheduler.StepLR(G_Optim, step_size= 20)
        lr_scheduler_D = torch.optim.lr_scheduler.StepLR(D_Optim, step_size = 20)

        return [G_Optim, D_Optim], [lr_scheduler_G, lr_scheduler_D]

    def training_step(self, batch, batch_idx): #should also take batch_idx

        """To train one batch"""
        batch = batch.to(device)
        G_batch = batch[:,:4,:,:]
        G_target_ims = batch[:,4,:,:]
        D_batch = batch
        G_Optim, D_Optim = self.optimizers()
        epoch = self.trainer.current_epoch

        ######
        # Generate fake sequence 
        ######

        log_utils.write_log("Starting to generate a batch of images")
        generated_sequences = self.forward(G_batch)

        # Log sequence to MLflow
        plot_seq = generated_sequences[-1] # plot the last generated sequence of each batch
        target_im = G_target_ims[-1]
        fig = self.save_figures(plot_seq, target_im)
        mlflow.log_figure(fig, f"plots/train/{epoch}/batch_{batch_idx}.png")

        # Log histograms to MLflow
        histogram = self.save_val_distribution(plot_seq[4])
        mlflow.log_figure(histogram, f"histograms/train/{epoch}/batch_{batch_idx}.png")
        plt.close()

        # log metrics to MLflow
        mlflow.log_metric("min_predicted_val", plot_seq[4].detach().min())
        mlflow.log_metric("max_predicted_val", plot_seq[4].detach().max())
        log_utils.write_log("Images have been generated by the generator")

        # Discriminate fake sequence 
        log_utils.write_log("Discriminator is being trained on fake data")
        fake_pred = self.discriminator(generated_sequences.detach())
        mlflow.log_metric("D_pred_on_fake_batch_average", torch.mean(fake_pred.detach()))

        # Optimize G
        log_utils.write_log("Optimization of G has started")
        G_Optim.zero_grad() # gradient reset
        reg_config = config_utils.get_pipeline_config().get('regularization', {}) # whether gcr was chosen
        if reg_config.get('grid_cell_reg') == 'True':
            grid_regularizer = grid_cell_regularizer(generated_sequences[:,4,:,:], G_target_ims)
            G_loss = generator_loss(fake_pred.squeeze()) + 20 * grid_regularizer # 20 is grid lambda
        elif reg_config.get('grid_cell_reg') == 'False':
            G_loss = generator_loss(fake_pred.squeeze()) 

        self.manual_backward(G_loss, retain_graph = True) # gradient computation
        G_Optim.step() # back-propagation

        # Optimize D on fake 
        log_utils.write_log("Discriminator is being optimized on fake data")
        D_loss_fake = discriminator_loss(fake_pred.squeeze(), target = 'fake') 
        self.manual_backward(D_loss_fake, retain_graph = True) # gradient computation
        D_Optim.step() # back-propagation

        # Optimize D on real 
        D_Optim.zero_grad() # gradient reset
        log_utils.write_log("Discriminator is being trained & optimized on real data")
        real_pred = self.discriminator(D_batch)
        mlflow.log_metric("D_pred_on_real_batch_average", torch.mean(real_pred.detach()))
        D_loss_real = discriminator_loss(real_pred.squeeze(), target = 'real')
        self.manual_backward(D_loss_real) # gradient computation

        # Step of learning rate scheduler
        sch_G, sch_D = self.lr_schedulers()
        sch_G.step()
        sch_D.step()
        if self.trainer.is_last_batch:
            sch_G.step()
            sch_D.step()

        ######
        # Log metrics 
        ######

        mlflow.log_metric("D_training_loss", D_loss_real.detach() + D_loss_fake.detach())
        mlflow.log_metric("G_training_loss", G_loss.detach())

        # Calculate SSIM score (SSIM between predicted and ground truth)
        ssim_score = SSIM(generated_sequences[:,4,:,:], G_target_ims).to(device)
        mlflow.log_metric("mean_batch_SSIM", ssim_score)

        # Calculate persisten baseline SSIM score (SSIM between observed 0 and 15 mins)
        persistent_ssim_score = SSIM(generated_sequences[:,3,:,:], G_target_ims).to(device)
        mlflow.log_metric("mean_persistent_SSIM", persistent_ssim_score)

        accuracy, macro_precision, micro_precision, _ = classification_metrics(generated_sequences[-1,4,:,:], G_target_ims[-1])
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("macro_precision", macro_precision)
        mlflow.log_metric("micro_precision", micro_precision)
    
    def validation_step(self, batch, batch_idx):

        batch = batch.to(device)
        G_batch = batch[:,:4,:,:] 
        G_target_ims = batch[:,4,:,:]
        D_batch = batch
        epoch = self.trainer.current_epoch

        # Generate sequence
        log_utils.write_log("Starting to generate a batch of validation images")
        generated_sequences = self.forward(G_batch)

        # Log sequence to MLflow
        plot_seq = generated_sequences[-1] # plot the last generated sequence of each batch
        target_im = G_target_ims[-1]
        fig = self.save_figures(plot_seq, target_im)
        mlflow.log_figure(fig, f"plots/val/epoch_{epoch}.png")

        # Log histograms to MLflow
        histogram = self.save_val_distribution(plot_seq[4])
        mlflow.log_figure(histogram, f"histograms/val/epoch_{epoch}.png")
        plt.close()

        # log metrics to MLflow
        mlflow.log_metric("min_predicted_val", plot_seq[4].detach().min())
        mlflow.log_metric("max_predicted_val", plot_seq[4].detach().max())

        log_utils.write_log("Validation images have been generated by the generator")
        
        # Let discriminator discriminate real and fake data
        log_utils.write_log("Discriminator is starting to discriminate")
        real_pred = self.discriminator(D_batch) # need some param that training = True
        fake_pred = self.discriminator(generated_sequences)

        # Calculate SSIM score (SSIM between predicted and ground truth)
        ssim_score = SSIM(generated_sequences[:,4,:,:], G_target_ims).to(device)
        mlflow.log_metric("val_mean_batch_SSIM", ssim_score)

        # Calculate persisten baseline SSIM score (SSIM between observed 0 and 15 mins)
        persistent_ssim_score = SSIM(generated_sequences[:,3,:,:], G_target_ims).to(device)
        mlflow.log_metric("mean_persistent_SSIM", persistent_ssim_score)

        accuracy, macro_precision, micro_precision, new_prediction = classification_metrics(generated_sequences[-1,4,:,:], G_target_ims[-1])
        mlflow.log_metric("val_accuracy", accuracy)
        mlflow.log_metric("val_macro_precision", macro_precision)
        mlflow.log_metric("val_micro_precision", micro_precision)

        # Log discretized sequence to MLflow
        plot_seq = torch.cat((generated_sequences[-1,:4,:,:], new_prediction.unsqueeze(dim=0)), dim = 0) # plot the last generated sequence of each batch
        target_im = G_target_ims[-1]
        fig = self.save_figures(plot_seq, target_im)
        mlflow.log_figure(fig, f"plots/val/epoch_{epoch}_km.png")

        # Calculate D loss
        D_loss_real = discriminator_loss(real_pred.squeeze(), target = 'real')
        D_loss_fake = discriminator_loss(fake_pred.squeeze().detach(), target = 'fake')
        mlflow.log_metric("D_validation_loss", D_loss_real + D_loss_fake)

        return #D_loss, G_loss 

    def test_step(self, batch, batch_idx):

        batch = batch.to(device)
        G_batch = batch[:,:4,:,:] 
        G_target_ims = batch[:,4,:,:]
        D_batch = batch
        epoch = self.trainer.current_epoch

        # Generate sequence
        generated_sequences = self.forward(G_batch)

        # Log sequence to MLflow
        plot_seq = generated_sequences[-1] # plot the last generated sequence of each batch
        target_im = G_target_ims[-1]
        fig = self.save_figures(plot_seq, target_im)
        mlflow.log_figure(fig, f"plots/test/epoch_{epoch}.png")

        # Log histograms to MLflow
        histogram = self.save_val_distribution(plot_seq[4])
        mlflow.log_figure(histogram, f"histograms/test/epoch_{epoch}.png")
        plt.close()

        # Calculate SSIM score (SSIM between predicted and ground truth)
        ssim_score = SSIM(generated_sequences[:,4,:,:], G_target_ims).to(device)
        mlflow.log_metric("test_mean_batch_SSIM", ssim_score)

        # Calculate persisten baseline SSIM score (SSIM between observed 0 and 15 mins)
        persistent_ssim_score = SSIM(generated_sequences[:,3,:,:], G_target_ims).to(device)
        mlflow.log_metric("mean_persistent_SSIM", persistent_ssim_score)

        accuracy, macro_precision, micro_precision, new_prediction = classification_metrics(generated_sequences[-1,4,:,:], G_target_ims[-1])
        mlflow.log_metric("test_accuracy", accuracy)
        mlflow.log_metric("test_macro_precision", macro_precision)
        mlflow.log_metric("test_micro_precision", micro_precision)

        # Log discretized sequence to MLflow
        plot_seq = torch.cat((generated_sequences[-1,:4,:,:], new_prediction.unsqueeze(dim=0)), dim = 0) # plot the last generated sequence of each batch
        target_im = G_target_ims[-1]
        fig = self.save_figures(plot_seq, target_im)
        mlflow.log_figure(fig, f"plots/test/epoch_{epoch}_km.png")

        return #D_loss, G_loss 
    
    def save_figures(self, sequence, target_im): # must have 5 images!

        # Creat custom color map 
        # cmap = colors.ListedColormap(['tab:blue', 'tab:green', 'white'])
        # bounds=[0,1,2,3]
        # norm = colors.BoundaryNorm(bounds, cmap.N)

        # Create a 1x5 grid of subplots
        fig, axes = plt.subplots(1, 6, figsize=(18, 4))  # Adjust the figsize as needed

        # Loop through the subplots and display the images
        for i, ax in enumerate(axes):
            # ax.imshow(predictions[prediction][i].detach(), cmap = cmap, norm = norm)
            if i<4:  
                ax.imshow(sequence[i].detach().cpu().numpy())
                ax.set_title(f"{int(45-15*i)} minutes before")
            elif i==4:
                ax.imshow(sequence[i].detach().cpu().numpy())
                ax.set_title("+ 15 minutes (generated)")
            elif i==5:
                ax.imshow(target_im.detach().cpu().numpy())
                ax.set_title("+ 15 minutes (ground truth)")

            ax.axis('off')  

        plt.tight_layout()  # Ensure proper spacing between subplots

        return fig
    
    def save_val_distribution(self, im): 

        im = im.detach().cpu().numpy()
        # Create a 1x5 grid of subplots
        fig, ax = plt.subplots()
        ax.hist(im.flatten(), bins = 100)
        ax.set_title("Value distribution in generated image")
        ax.set_ylabel("frequency")

        return fig